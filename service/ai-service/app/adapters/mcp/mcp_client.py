"""
MCP (Model Context Protocol) í´ë¼ì´ì–¸íŠ¸
ë‹¤ì–‘í•œ MCP ì„œë²„ë“¤ê³¼ í†µì‹ í•˜ì—¬ ê·¼ê±° ìˆ˜ì§‘
"""

import asyncio
import logging
import re
from typing import List, Dict, Any, Optional
from enum import Enum

logger = logging.getLogger(__name__)

class MCPServerType(str, Enum):
    """ì§€ì›í•˜ëŠ” MCP ì„œë²„ íƒ€ì…ë“¤"""
    BRAVE_SEARCH = "brave_search"
    TAVILY_SEARCH = "tavily_search"
    WIKIPEDIA = "wikipedia"
    ACADEMIC_SEARCH = "academic_search"
    GOOGLE_SEARCH = "google_search"

class MCPClient:
    """MCP ì„œë²„ë“¤ê³¼ í†µì‹ í•˜ëŠ” í´ë¼ì´ì–¸íŠ¸"""
    
    def __init__(self):
        self.server_configs = {
            MCPServerType.BRAVE_SEARCH: {
                'endpoint': 'brave-search-mcp',
                'tools': ['web_search']
            },
            MCPServerType.TAVILY_SEARCH: {
                'endpoint': 'tavily-mcp',
                'tools': ['search']
            },
            MCPServerType.WIKIPEDIA: {
                'endpoint': 'wikipedia-mcp',
                'tools': ['search_wikipedia']
            },
            MCPServerType.ACADEMIC_SEARCH: {
                'endpoint': 'academic-mcp',
                'tools': ['search_papers']
            },
            MCPServerType.GOOGLE_SEARCH: {
                'endpoint': 'google-search-mcp',
                'tools': ['web_search']
            }
        }
    
    def _preprocess_query_for_wikipedia(self, query: str) -> str:
        """
        Wikipedia ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ì „ì²˜ë¦¬
        ê¸´ ë¬¸ì¥ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œ
        """
        # ì—°ë„ íŒ¨í„´ ì œê±° (1950ë…„ëŒ€:, 1956ë…„: ë“±)
        query = re.sub(r'^\d{4}ë…„ëŒ€?:\s*', '', query)
        
        # ì¸ëª… ì¶”ì¶œ (í•œê¸€ ì´ë¦„, ì˜ë¬¸ ì´ë¦„)
        names = re.findall(r'[A-Z][a-z]+\s+[A-Z][a-z]+|[ê°€-í£]{2,4}(?:\s+[ê°€-í£]{2,4})?', query)
        if names:
            # ì²« ë²ˆì§¸ ì¸ëª… ì‚¬ìš©
            return names[0]
        
        # ê³ ìœ ëª…ì‚¬/ì „ë¬¸ìš©ì–´ ì¶”ì¶œ (ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ê±°ë‚˜ íŠ¹ì • íŒ¨í„´)
        # ë‹¤íŠ¸ë¨¸ìŠ¤ ì»¨í¼ëŸ°ìŠ¤, ì „ë¬¸ê°€ ì‹œìŠ¤í…œ, ë”¥ëŸ¬ë‹ ë“±
        terms = re.findall(r'[ê°€-í£A-Z][ê°€-í£a-z\s]{2,15}(?:ì‹œìŠ¤í…œ|ì»¨í¼ëŸ°ìŠ¤|ê¸°ìˆ |ì ‘ê·¼ë²•|í•™ìŠµ)', query)
        if terms:
            return terms[0].strip()
        
        # í•µì‹¬ ëª…ì‚¬êµ¬ ì¶”ì¶œ (ì¡°ì‚¬ ì œê±°)
        # "ì¸ê³µì§€ëŠ¥ì´ë¼ëŠ”", "ê¸°ê³„í•™ìŠµ ê¸°ìˆ ì˜" -> "ì¸ê³µì§€ëŠ¥", "ê¸°ê³„í•™ìŠµ"
        nouns = re.findall(r'([ê°€-í£]{2,10})(?:ì´ë¼ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì—ì„œ|ìœ¼ë¡œ|ë¼ëŠ”)', query)
        if nouns:
            return nouns[0]
        
        # ì˜ë¬¸ ì „ë¬¸ìš©ì–´ ì¶”ì¶œ
        english_terms = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*', query)
        if english_terms:
            return english_terms[0]
        
        # ê¸°ë³¸: ì²« 20ì + í•µì‹¬ ë‹¨ì–´ë§Œ
        words = query.split()
        if len(words) > 3:
            # ì¡°ì‚¬ ì œê±°í•˜ê³  ëª…ì‚¬ë§Œ
            keywords = [w for w in words[:5] if len(w) > 1 and not w.endswith(('ì€', 'ëŠ”', 'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì˜', 'ì—', 'ë¡œ'))]
            return ' '.join(keywords[:3])
        
        return query[:30]
    
    def _preprocess_query_for_academic(self, query: str) -> str:
        """
        í•™ìˆ  ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ì „ì²˜ë¦¬
        ì˜ë¬¸ í‚¤ì›Œë“œ ì¶”ì¶œ ë° ë²ˆì—­
        """
        # ì—°ë„ íŒ¨í„´ ì œê±°
        query = re.sub(r'^\d{4}ë…„ëŒ€?:\s*', '', query)
        
        # í•œê¸€ ì „ë¬¸ìš©ì–´ -> ì˜ë¬¸ ë§¤í•‘
        term_mapping = {
            'ì¸ê³µì§€ëŠ¥': 'artificial intelligence',
            'ê¸°ê³„í•™ìŠµ': 'machine learning',
            'ë”¥ëŸ¬ë‹': 'deep learning',
            'ì‹ ê²½ë§': 'neural network',
            'ìì—°ì–´ ì²˜ë¦¬': 'natural language processing',
            'ì»´í“¨í„° ë¹„ì „': 'computer vision',
            'ì „ë¬¸ê°€ ì‹œìŠ¤í…œ': 'expert system',
            'ê¸°í˜¸ì£¼ì˜': 'symbolic AI',
            'ë¹…ë°ì´í„°': 'big data'
        }
        
        # ë§¤í•‘ëœ ìš©ì–´ ì°¾ê¸°
        for ko, en in term_mapping.items():
            if ko in query:
                return en
        
        # ì˜ë¬¸ì´ ì´ë¯¸ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        english_terms = re.findall(r'[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*', query)
        if english_terms:
            return ' '.join(english_terms[:3])
        
        # ê¸°ë³¸: í•µì‹¬ ëª…ì‚¬ë§Œ
        nouns = re.findall(r'([ê°€-í£]{2,10})(?:ì´ë¼ëŠ”|ì´|ê°€|ì„|ë¥¼|ì˜|ì—ì„œ|ìœ¼ë¡œ)', query)
        return nouns[0] if nouns else query[:30]
    
    async def search_evidence(
        self, 
        server_type: MCPServerType, 
        query: str, 
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        ì§€ì •ëœ MCP ì„œë²„ì—ì„œ ê·¼ê±° ê²€ìƒ‰
        
        Args:
            server_type: ì‚¬ìš©í•  MCP ì„œë²„ íƒ€ì…
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            limit: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ëœ ê·¼ê±° ë¦¬ìŠ¤íŠ¸
        """
        import time
        
        try:
            # ì„œë²„ íƒ€ì…ë³„ ì¿¼ë¦¬ ì „ì²˜ë¦¬
            processed_query = query
            if server_type == MCPServerType.WIKIPEDIA:
                processed_query = self._preprocess_query_for_wikipedia(query)
                logger.debug(f"    ğŸ”· [MCP] Wikipedia ì¿¼ë¦¬ ì „ì²˜ë¦¬: '{query[:30]}...' -> '{processed_query}'")
            elif server_type == MCPServerType.ACADEMIC_SEARCH:
                processed_query = self._preprocess_query_for_academic(query)
                logger.debug(f"    ğŸ”· [MCP] Academic ì¿¼ë¦¬ ì „ì²˜ë¦¬: '{query[:30]}...' -> '{processed_query}'")
            
            logger.debug(f"    ğŸ”· [MCP Tool] í˜¸ì¶œ ì‹œì‘")
            logger.debug(f"       ë„êµ¬ëª…: {server_type.value}")
            logger.debug(f"       ê²€ìƒ‰ì–´: {processed_query[:50]}...")
            logger.debug(f"       ê²°ê³¼ ì œí•œ: {limit}ê°œ")
            
            start_time = time.time()
            
            # ì„œë²„ íƒ€ì…ë³„ ê²€ìƒ‰ ì‹¤í–‰
            if server_type == MCPServerType.BRAVE_SEARCH:
                results = await self._search_brave(processed_query, limit)
            elif server_type == MCPServerType.TAVILY_SEARCH:
                results = await self._search_tavily(processed_query, limit)
            elif server_type == MCPServerType.WIKIPEDIA:
                results = await self._search_wikipedia(processed_query, limit)
            elif server_type == MCPServerType.ACADEMIC_SEARCH:
                results = await self._search_academic(processed_query, limit)
            elif server_type == MCPServerType.GOOGLE_SEARCH:
                results = await self._search_google(processed_query, limit)
            else:
                logger.error(f"Unsupported MCP server type: {server_type}")
                return []
            
            elapsed = time.time() - start_time
            
            logger.debug(f"    âœ… [MCP Tool] í˜¸ì¶œ ì„±ê³µ")
            logger.debug(f"       ì‹¤í–‰ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            logger.debug(f"       ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ")
            if results:
                logger.debug(f"       ìƒìœ„ ê²°ê³¼: {results[0].get('title', '')[:50]}...")
            
            return results
                
        except Exception as e:
            logger.error(f"    âŒ [MCP Tool] í˜¸ì¶œ ì‹¤íŒ¨")
            logger.error(f"       ë„êµ¬: {server_type.value}")
            logger.error(f"       ì—ëŸ¬: {str(e)}")
            return []
    
    async def _search_brave(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Brave Search MCP ì„œë²„ í˜¸ì¶œ - ì‹¤ì œ êµ¬í˜„"""
        import aiohttp
        import os
        
        api_key = os.getenv("BRAVE_API_KEY")
        if not api_key:
            logger.error("BRAVE_API_KEY not found - cannot perform search")
            raise ValueError("BRAVE_API_KEY environment variable is required")
        
        url = "https://api.search.brave.com/res/v1/web/search"
        headers = {
            "Accept": "application/json",
            "X-Subscription-Token": api_key
        }
        params = {
            "q": query,
            "count": min(limit, 20),
            "search_lang": "ko",
            "country": "KR"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, headers=headers, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    results = []
                    
                    for item in data.get("web", {}).get("results", [])[:limit]:
                        results.append({
                            'title': item.get('title', ''),
                            'content': item.get('description', ''),
                            'url': item.get('url', ''),
                            'source': 'brave_search',
                            'relevance_score': 0.9,
                            'published_date': item.get('age', ''),
                            'domain': item.get('profile', {}).get('name', '')
                        })
                    
                    logger.info(f"Brave Search returned {len(results)} results")
                    return results
                else:
                    error_msg = f"Brave Search API error: {response.status}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
    
    async def _search_tavily(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Tavily Search MCP ì„œë²„ í˜¸ì¶œ - ì‹¤ì œ êµ¬í˜„"""
        import aiohttp
        import os
        
        api_key = os.getenv("TAVILY_API_KEY")
        if not api_key:
            logger.error("TAVILY_API_KEY not found - cannot perform search")
            raise ValueError("TAVILY_API_KEY environment variable is required")
        
        url = "https://api.tavily.com/search"
        headers = {"Content-Type": "application/json"}
        payload = {
            "api_key": api_key,
            "query": query,
            "search_depth": "basic",
            "include_answer": False,
            "include_images": False,
            "include_raw_content": False,
            "max_results": min(limit, 10)
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, headers=headers, json=payload) as response:
                if response.status == 200:
                    data = await response.json()
                    results = []
                    
                    for item in data.get("results", [])[:limit]:
                        results.append({
                            'title': item.get('title', ''),
                            'content': item.get('content', ''),
                            'url': item.get('url', ''),
                            'source': 'tavily_search',
                            'relevance_score': item.get('score', 0.5),
                            'confidence': 0.9,
                            'category': 'general'
                        })
                    
                    logger.info(f"Tavily Search returned {len(results)} results")
                    return results
                else:
                    error_msg = f"Tavily Search API error: {response.status}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
    
    async def _search_wikipedia(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Wikipedia MCP ì„œë²„ í˜¸ì¶œ - ì‹¤ì œ êµ¬í˜„"""
        import aiohttp
        import urllib.parse
        
        # í•œêµ­ì–´ ìœ„í‚¤í”¼ë””ì•„ ìš°ì„  ê²€ìƒ‰
        encoded_query = urllib.parse.quote(query)
        
        # Wikipedia APIëŠ” User-Agent í•„ìˆ˜
        headers = {
            "User-Agent": "FromPromAI/1.0 (https://fromprom.ai; contact@fromprom.ai)"
        }
        
        async with aiohttp.ClientSession(headers=headers) as session:
            # ë¨¼ì € ìš”ì•½ API ì‹œë„
            summary_url = f"https://ko.wikipedia.org/api/rest_v1/page/summary/{encoded_query}"
            
            try:
                async with session.get(summary_url) as response:
                    if response.status == 200:
                        data = await response.json()
                        return [{
                            'title': data.get('title', ''),
                            'content': data.get('extract', ''),
                            'url': data.get('content_urls', {}).get('desktop', {}).get('page', ''),
                            'source': 'wikipedia',
                            'relevance_score': 0.95,
                            'language': 'ko',
                            'page_id': str(data.get('pageid', ''))
                        }]
            except:
                pass
            
            # ê²€ìƒ‰ APIë¡œ ëŒ€ì²´
            search_url = "https://ko.wikipedia.org/w/api.php"
            params = {
                "action": "query",
                "format": "json",
                "list": "search",
                "srsearch": query,
                "srlimit": min(limit, 5),
                "srprop": "snippet|titlesnippet"
            }
            
            async with session.get(search_url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    results = []
                    
                    for item in data.get("query", {}).get("search", []):
                        title = item.get('title', '')
                        results.append({
                            'title': f'Wikipedia: {title}',
                            'content': item.get('snippet', '').replace('<span class="searchmatch">', '').replace('</span>', ''),
                            'url': f"https://ko.wikipedia.org/wiki/{urllib.parse.quote(title)}",
                            'source': 'wikipedia',
                            'relevance_score': 0.9,
                            'language': 'ko',
                            'page_id': str(item.get('pageid', ''))
                        })
                    
                    logger.info(f"Wikipedia returned {len(results)} results")
                    return results
                else:
                    error_msg = f"Wikipedia API error: {response.status}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
    
    async def _search_academic(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Academic Paper MCP ì„œë²„ í˜¸ì¶œ - arXiv API ì‚¬ìš©"""
        import aiohttp
        import xml.etree.ElementTree as ET
        import urllib.parse
        
        # arXiv API (ë¬´ë£Œ, Rate Limit ê´€ëŒ€: ì´ˆë‹¹ 1íšŒ)
        encoded_query = urllib.parse.quote(query)
        url = f"http://export.arxiv.org/api/query?search_query=all:{encoded_query}&start=0&max_results={min(limit, 10)}"
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url) as response:
                if response.status == 200:
                    xml_data = await response.text()
                    
                    # XML íŒŒì‹±
                    root = ET.fromstring(xml_data)
                    ns = {'atom': 'http://www.w3.org/2005/Atom'}
                    
                    results = []
                    for entry in root.findall('atom:entry', ns):
                        title = entry.find('atom:title', ns)
                        summary = entry.find('atom:summary', ns)
                        link = entry.find('atom:id', ns)
                        published = entry.find('atom:published', ns)
                        
                        authors = []
                        for author in entry.findall('atom:author', ns):
                            name = author.find('atom:name', ns)
                            if name is not None:
                                authors.append(name.text)
                        
                        results.append({
                            'title': title.text.strip() if title is not None else '',
                            'content': summary.text.strip() if summary is not None else '',
                            'url': link.text if link is not None else '',
                            'source': 'arxiv',
                            'relevance_score': 0.88,
                            'authors': ', '.join(authors),
                            'publication_date': published.text[:10] if published is not None else '',
                            'journal': 'arXiv',
                            'citations': 0
                        })
                    
                    logger.info(f"arXiv returned {len(results)} results")
                    return results
                else:
                    error_msg = f"arXiv API error: {response.status}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
    
    async def _search_google(self, query: str, limit: int) -> List[Dict[str, Any]]:
        """Google Custom Search API í˜¸ì¶œ - ì‹¤ì œ êµ¬í˜„"""
        import aiohttp
        import os
        
        api_key = os.getenv("GOOGLE_SEARCH_API_KEY")
        search_engine_id = os.getenv("GOOGLE_SEARCH_ENGINE_ID")
        
        if not api_key or not search_engine_id:
            logger.error("GOOGLE_SEARCH_API_KEY or GOOGLE_SEARCH_ENGINE_ID not found - cannot perform search")
            raise ValueError("GOOGLE_SEARCH_API_KEY and GOOGLE_SEARCH_ENGINE_ID environment variables are required")
        
        # Google Custom Search API
        url = "https://www.googleapis.com/customsearch/v1"
        params = {
            "key": api_key,
            "cx": search_engine_id,
            "q": query,
            "num": min(limit, 10),
            "lr": "lang_ko",  # í•œêµ­ì–´ ìš°ì„ 
            "safe": "medium"
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    results = []
                    
                    for item in data.get("items", [])[:limit]:
                        results.append({
                            'title': item.get('title', ''),
                            'content': item.get('snippet', ''),
                            'url': item.get('link', ''),
                            'source': 'google_search',
                            'relevance_score': 0.9,
                            'display_link': item.get('displayLink', ''),
                            'formatted_url': item.get('formattedUrl', ''),
                            'cache_id': item.get('cacheId', '')
                        })
                    
                    logger.info(f"Google Search returned {len(results)} results")
                    return results
                else:
                    error_msg = f"Google Search API error: {response.status}"
                    logger.error(error_msg)
                    raise RuntimeError(error_msg)
    
