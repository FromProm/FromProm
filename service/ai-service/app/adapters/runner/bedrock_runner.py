import json
import logging
import boto3
import asyncio
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, Any
from app.adapters.runner.base import BaseRunner
from app.core.config import settings
from app.core.errors import ModelInvocationError

logger = logging.getLogger(__name__)

class BedrockRunner(BaseRunner):
    """AWS Bedrock ëª¨ë¸ ì‹¤í–‰ê¸° (ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)"""
    
    def __init__(self):
        # boto3 ì„¤ì • (ì—°ê²° í’€ í¬ê¸° ì¦ê°€)
        from botocore.config import Config
        config = Config(
            max_pool_connections=25,  # ì—°ê²° í’€ í¬ê¸° ì¦ê°€
            retries={'max_attempts': 3}
        )
        
        self.client = boto3.client(
            'bedrock-runtime',
            region_name=settings.aws_region,
            aws_access_key_id=settings.aws_access_key_id or None,
            aws_secret_access_key=settings.aws_secret_access_key or None,
            config=config
        )
        # ìŠ¤ë ˆë“œí’€ ìƒì„± (ë³‘ë ¬ ì²˜ë¦¬ìš©)
        self.executor = ThreadPoolExecutor(max_workers=20)
    
    async def invoke(
        self, 
        model: str, 
        prompt: str, 
        input_type: str = "text",
        **kwargs
    ) -> Dict[str, Any]:
        """Bedrock ëª¨ë¸ í˜¸ì¶œ (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬)"""
        try:
            logger.info(f"Invoking model: {model}")
            
            # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                self.executor,
                self._sync_invoke,
                model, prompt, input_type, kwargs
            )
            return result
            
        except Exception as e:
            logger.error(f"Model invocation failed for {model}: {str(e)}")
            raise ModelInvocationError(f"Failed to invoke {model}: {str(e)}")
    
    def _sync_invoke(self, model: str, prompt: str, input_type: str, kwargs: dict) -> Dict[str, Any]:
        """ë™ê¸° Bedrock í˜¸ì¶œ (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰)"""
        import time
        
        try:
            logger.debug(f"    ğŸ”· [Bedrock LLM] í˜¸ì¶œ ì‹œì‘")
            logger.debug(f"       ëª¨ë¸ ID: {model}")
            logger.debug(f"       í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {len(prompt)} ë¬¸ì")
            
            start_time = time.time()
            
            # inference profile ARNì¸ ê²½ìš° converse API ì‚¬ìš©
            if model.startswith("arn:aws:bedrock"):
                result = self._invoke_with_converse(model, prompt, **kwargs)
                elapsed = time.time() - start_time
                logger.debug(f"    âœ… [Bedrock LLM] Converse API í˜¸ì¶œ ì„±ê³µ")
                logger.debug(f"       ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ")
                logger.debug(f"       ì…ë ¥ í† í°: {result['token_usage']['input_tokens']}")
                logger.debug(f"       ì¶œë ¥ í† í°: {result['token_usage']['output_tokens']}")
                return result
            
            # ëª¨ë¸ë³„ ìš”ì²­ í˜•ì‹ êµ¬ì„±
            if "anthropic.claude" in model:
                body = self._build_claude_request(prompt, **kwargs)
            elif "openai.gpt" in model:
                body = self._build_openai_request(prompt, **kwargs)
            elif "google.gemma" in model:
                return self._invoke_with_converse(model, prompt, **kwargs)
            elif "amazon.titan" in model:
                if "image" in model:
                    # Titan Image Generator
                    body = self._build_titan_image_request(prompt, **kwargs)
                else:
                    # Titan Text
                    body = self._build_titan_request(prompt, **kwargs)
            elif "amazon.nova" in model:
                body = self._build_nova_request(model, prompt, input_type, **kwargs)
            else:
                raise ModelInvocationError(f"Unsupported model: {model}")
            
            # Bedrock í˜¸ì¶œ
            response = self.client.invoke_model(
                modelId=model,
                body=json.dumps(body),
                contentType='application/json'
            )
            
            elapsed = time.time() - start_time
            
            # ì‘ë‹µ íŒŒì‹±
            response_body = json.loads(response['body'].read())
            
            # ëª¨ë¸ë³„ ì‘ë‹µ íŒŒì‹±
            if "anthropic.claude" in model:
                result = self._parse_claude_response(response_body)
            elif "openai.gpt" in model:
                result = self._parse_openai_response(response_body)
            elif "amazon.titan" in model:
                if "image" in model:
                    result = self._parse_titan_image_response(response_body)
                else:
                    result = self._parse_titan_response(response_body)
            elif "amazon.nova" in model:
                result = self._parse_nova_response(response_body, model)
            
            logger.debug(f"    âœ… [Bedrock LLM] í˜¸ì¶œ ì„±ê³µ")
            logger.debug(f"       ì‘ë‹µ ì‹œê°„: {elapsed:.2f}ì´ˆ")
            logger.debug(f"       ì…ë ¥ í† í°: {result['token_usage']['input_tokens']}")
            logger.debug(f"       ì¶œë ¥ í† í°: {result['token_usage']['output_tokens']}")
            
            return result
            
        except Exception as e:
            logger.error(f"    âŒ [Bedrock LLM] í˜¸ì¶œ ì‹¤íŒ¨")
            logger.error(f"       ëª¨ë¸: {model}")
            logger.error(f"       ì—ëŸ¬: {str(e)}")
            raise ModelInvocationError(f"Failed to invoke {model}: {str(e)}")
    
    def _invoke_with_converse(self, model_arn: str, prompt: str, **kwargs) -> Dict[str, Any]:
        """Converse APIë¥¼ ì‚¬ìš©í•œ inference profile í˜¸ì¶œ"""
        try:
            response = self.client.converse(
                modelId=model_arn,
                messages=[
                    {
                        "role": "user",
                        "content": [{"text": prompt}]
                    }
                ],
                inferenceConfig={
                    "maxTokens": kwargs.get('max_tokens', 1000),
                    "temperature": kwargs.get('temperature', 0.7)
                }
            )
            
            # ì‘ë‹µ íŒŒì‹±
            output_message = response.get('output', {}).get('message', {})
            content = output_message.get('content', [])
            output_text = content[0].get('text', '') if content else ''
            
            usage = response.get('usage', {})
            token_usage = {
                'input_tokens': usage.get('inputTokens', 0),
                'output_tokens': usage.get('outputTokens', 0),
                'total_tokens': usage.get('inputTokens', 0) + usage.get('outputTokens', 0)
            }
            
            return {
                'output': output_text,
                'token_usage': token_usage
            }
            
        except Exception as e:
            logger.error(f"Converse API failed: {str(e)}")
            raise ModelInvocationError(f"Failed to invoke with converse: {str(e)}")
    
    def _build_claude_request(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Claude ìš”ì²­ êµ¬ì„±"""
        return {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": kwargs.get('max_tokens', 1000),
            "temperature": kwargs.get('temperature', 0.7),
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }
    
    def _build_openai_request(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """OpenAI GPT OSS ìš”ì²­ êµ¬ì„±"""
        return {
            "messages": [
                {
                    "role": "system",
                    "content": "You are a helpful assistant."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "max_completion_tokens": kwargs.get('max_tokens', 1000),
            "temperature": kwargs.get('temperature', 0.7),
            "top_p": kwargs.get('top_p', 0.9)
        }
    
    def _build_titan_request(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Titan ìš”ì²­ êµ¬ì„±"""
        return {
            "inputText": prompt,
            "textGenerationConfig": {
                "maxTokenCount": kwargs.get('max_tokens', 1000),
                "temperature": kwargs.get('temperature', 0.7),
                "topP": kwargs.get('top_p', 0.9)
            }
        }
    
    def _parse_claude_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Claude ì‘ë‹µ íŒŒì‹±"""
        content = response.get('content', [])
        output_text = content[0].get('text', '') if content else ''
        
        usage = response.get('usage', {})
        token_usage = {
            'input_tokens': usage.get('input_tokens', 0),
            'output_tokens': usage.get('output_tokens', 0),
            'total_tokens': usage.get('input_tokens', 0) + usage.get('output_tokens', 0)
        }
        
        return {
            'output': output_text,
            'token_usage': token_usage
        }
    
    def _parse_openai_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """OpenAI GPT OSS ì‘ë‹µ íŒŒì‹±"""
        choices = response.get('choices', [])
        output_text = ''
        if choices:
            message = choices[0].get('message', {})
            output_text = message.get('content', '')
        
        usage = response.get('usage', {})
        token_usage = {
            'input_tokens': usage.get('prompt_tokens', 0),
            'output_tokens': usage.get('completion_tokens', 0),
            'total_tokens': usage.get('total_tokens', 0)
        }
        
        return {
            'output': output_text,
            'token_usage': token_usage
        }
    
    def _parse_titan_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Titan ì‘ë‹µ íŒŒì‹±"""
        results = response.get('results', [])
        output_text = results[0].get('outputText', '') if results else ''
        
        # Titanì€ í† í° ì‚¬ìš©ëŸ‰ì„ ì§ì ‘ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê·¼ì‚¬ì¹˜ ê³„ì‚°
        input_tokens = len(response.get('inputText', '').split()) * 1.3  # ê·¼ì‚¬ì¹˜
        output_tokens = len(output_text.split()) * 1.3
        
        token_usage = {
            'input_tokens': int(input_tokens),
            'output_tokens': int(output_tokens),
            'total_tokens': int(input_tokens + output_tokens)
        }
        
        return {
            'output': output_text,
            'token_usage': token_usage
        }
    
    def _build_nova_request(self, model: str, prompt: str, input_type: str, **kwargs) -> Dict[str, Any]:
        """Nova ìš”ì²­ êµ¬ì„±"""
        if "canvas" in model:
            # Nova Canvas (ì´ë¯¸ì§€ ìƒì„±)
            return {
                "taskType": "TEXT_IMAGE",
                "textToImageParams": {
                    "text": prompt,
                    "negativeText": kwargs.get('negative_prompt', 'blurry, low quality, distorted')
                },
                "imageGenerationConfig": {
                    "numberOfImages": kwargs.get('number_of_images', 1),
                    "quality": kwargs.get('quality', 'standard'),
                    "cfgScale": kwargs.get('cfg_scale', 8.0),
                    "height": kwargs.get('height', 1024),
                    "width": kwargs.get('width', 1024),
                    "seed": kwargs.get('seed', 0)
                }
            }
        else:
            raise ModelInvocationError(f"Unsupported Nova model: {model}")
    
    def _build_titan_image_request(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Titan Image Generator ìš”ì²­ êµ¬ì„±"""
        return {
            "taskType": "TEXT_IMAGE",
            "textToImageParams": {
                "text": prompt,
                "negativeText": kwargs.get('negative_prompt', 'blurry, low quality, distorted')
            },
            "imageGenerationConfig": {
                "numberOfImages": kwargs.get('number_of_images', 1),
                "quality": kwargs.get('quality', 'standard'),
                "cfgScale": kwargs.get('cfg_scale', 8.0),
                "height": kwargs.get('height', 1024),
                "width": kwargs.get('width', 1024),
                "seed": kwargs.get('seed', 0)
            }
        }
    
    def _parse_nova_response(self, response: Dict[str, Any], model: str) -> Dict[str, Any]:
        """Nova ì‘ë‹µ íŒŒì‹±"""
        if "canvas" in model:
            # Nova Canvas ì´ë¯¸ì§€ ì‘ë‹µ
            images = response.get('images', [])
            
            if images:
                # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ base64 ë°ì´í„°ë¥¼ ë°˜í™˜
                output_text = images[0]  # base64 ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë°˜í™˜
                logger.info(f"Nova Canvas generated {len(images)} image(s)")
            else:
                output_text = ""
                logger.warning("No images generated by Nova Canvas")
        else:
            output_text = "Unknown Nova model response"
        
        # í† í° ì‚¬ìš©ëŸ‰ (NovaëŠ” ì§ì ‘ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê·¼ì‚¬ì¹˜)
        input_tokens = 50  # ê¸°ë³¸ê°’
        output_tokens = 100  # ì´ë¯¸ì§€ ìƒì„±ì€ ê³ ì •ê°’
        
        token_usage = {
            'input_tokens': int(input_tokens),
            'output_tokens': int(output_tokens),
            'total_tokens': int(input_tokens + output_tokens)
        }
        
        return {
            'output': output_text,
            'token_usage': token_usage
        }
    def _parse_titan_image_response(self, response: Dict[str, Any]) -> Dict[str, Any]:
        """Titan Image Generator ì‘ë‹µ íŒŒì‹±"""
        images = response.get('images', [])
        
        if images:
            # ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì˜ base64 ë°ì´í„°ë¥¼ ë°˜í™˜
            output_text = images[0]  # base64 ë¬¸ìì—´ ê·¸ëŒ€ë¡œ ë°˜í™˜
            logger.info(f"Titan Image generated {len(images)} image(s)")
        else:
            output_text = ""
            logger.warning("No images generated by Titan Image")
        
        # í† í° ì‚¬ìš©ëŸ‰ (Titan ImageëŠ” ì§ì ‘ ì œê³µí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ê·¼ì‚¬ì¹˜)
        input_tokens = 50  # ê¸°ë³¸ê°’
        output_tokens = 100  # ì´ë¯¸ì§€ ìƒì„±ì€ ê³ ì •ê°’
        
        token_usage = {
            'input_tokens': int(input_tokens),
            'output_tokens': int(output_tokens),
            'total_tokens': int(input_tokens + output_tokens)
        }
        
        return {
            'output': output_text,
            'token_usage': token_usage
        }