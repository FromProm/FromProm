"""
Strands Framework ê¸°ë°˜ Supervisor Agent
"""

import asyncio
import logging
import time
import uuid
from typing import Dict, Any, List, Optional

from app.core.schemas import JobCreateRequest, EvaluationResult, PromptType, MetricScore, TokenMetricScore
from app.orchestrator.context import ExecutionContext
from app.agents.strands.agent_core import create_agent_core
from app.core.logging import format_duration
from app.adapters.user.user_repository import UserRepository
from app.adapters.notification.ses_notifier import SESNotifier

logger = logging.getLogger(__name__)

class StrandsSupervisorAgent:
    """
    Strands Framework ê¸°ë°˜ Supervisor Agent
    """

    def __init__(self, context: ExecutionContext):
        self.context = context
        self.strands_core = None
        self.user_repo = UserRepository()  # User Repository ì¶”ê°€
        self.ses_notifier = SESNotifier()  # SES Notifier ì¶”ê°€
        self._initialize()
    
    def _initialize(self):
        """ì´ˆê¸°í™”"""
        try:
            self.strands_core = create_agent_core()
            logger.info("ğŸ¯ Strands Supervisor initialized")
        except Exception as e:
            logger.error(f"Failed to initialize Strands: {str(e)}")
            raise
    
    async def evaluate_prompt(self, job_request: JobCreateRequest) -> EvaluationResult:
        """
        Strands ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ í‰ê°€
        """
        # ì‹¤í–‰ ID ìƒì„± (ê° ì‹¤í–‰ì„ êµ¬ë¶„í•˜ê¸° ìœ„í•¨)
        execution_id = str(uuid.uuid4())[:8]
        
        supervisor_start = time.time()
        logger.info(f"[{execution_id}] ğŸ¯ Strands Supervisor starting evaluation for: {job_request.prompt_type}")

        try:
            if not self.strands_core or not self.strands_core.is_available():
                raise Exception("Strands Agent Core not available")

            # 1. ê¸°ë³¸ ë°ì´í„° ì¤€ë¹„
            step1_start = time.time()
            logger.info(f"[{execution_id}] ğŸ“‹ Step 1: Preparing execution data...")
            execution_data = await self._prepare_execution_data(job_request)
            step1_duration = time.time() - step1_start
            logger.info(f"[{execution_id}] ğŸ“‹ Step 1 Complete (AI ëª¨ë¸ ì‹¤í–‰ ë° ì„ë² ë”© ìƒì„±) - {format_duration(step1_duration)}")

            # 2. Agent ì„ íƒ
            step2_start = time.time()
            logger.info(f"[{execution_id}] ğŸ¤– Step 2: Selecting agents...")
            agent_types = self._select_agents(job_request.prompt_type)
            step2_duration = time.time() - step2_start
            logger.info(f"[{execution_id}] ğŸ¤– Step 2 Complete (í”„ë¡¬í”„íŠ¸ íƒ€ì…ë³„ í‰ê°€ ì§€í‘œ ì„ íƒ) - {format_duration(step2_duration)}")

            # 3. Workflow ì‹¤í–‰
            step3_start = time.time()
            logger.info(f"[{execution_id}] âš¡ Step 3: Dispatching workers (sending requests to metric calculators)...")
            workflow_results = await self._execute_workflow(
                agent_types, job_request, execution_data, execution_id
            )
            step3_duration = time.time() - step3_start
            logger.info(f"[{execution_id}] âš¡ Step 3 Complete (6ê°œ ì§€í‘œ ë³‘ë ¬ ê³„ì‚° ì™„ë£Œ) - {format_duration(step3_duration)}")

            # 4. ê²°ê³¼ í†µí•©
            step4_start = time.time()
            logger.info(f"[{execution_id}] ğŸ“Š Step 4: Integrating results...")
            final_score, weighted_scores, metrics = await self._integrate_results(
                workflow_results, job_request.prompt_type
            )
            step4_duration = time.time() - step4_start
            logger.info(f"[{execution_id}] ğŸ“Š Step 4 Complete (ê°€ì¤‘ì¹˜ ì ìš© ë° ìµœì¢… ì ìˆ˜ ê³„ì‚°) - {format_duration(step4_duration)}")

            # 5. í”¼ë“œë°± ìƒì„±
            step5_start = time.time()
            logger.info(f"[{execution_id}] ğŸ’¬ Step 5: Generating feedback...")
            feedback = await self._generate_feedback(job_request, metrics, final_score)
            step5_duration = time.time() - step5_start
            logger.info(f"[{execution_id}] ğŸ’¬ Step 5 Complete (AI í”¼ë“œë°± ë° ê°œì„  ì œì•ˆ ìƒì„±) - {format_duration(step5_duration)}")

            evaluation_result = EvaluationResult(
                final_score=final_score,
                weighted_scores=weighted_scores,
                execution_results=execution_data["execution_results"],
                feedback=feedback,
                **metrics
            )

            total_duration = time.time() - supervisor_start
            logger.info(f"[{execution_id}] âœ… Supervisor completed - Final Score: {final_score} - Total: {format_duration(total_duration)}")

            # âœ¨ ì´ë©”ì¼ ë°œì†¡ (PKê°€ ìˆëŠ” ê²½ìš°)
            if job_request.PK:
                await self._send_completion_email(
                    job_request=job_request,
                    final_score=final_score,
                    execution_id=execution_id
                )

            return evaluation_result

        except Exception as e:
            logger.error(f"[{execution_id}] âŒ Strands Supervisor failed: {str(e)}")
            raise
    
    def _select_agents(self, prompt_type: PromptType) -> List[str]:
        """í”„ë¡¬í”„íŠ¸ íƒ€ì…ì— ë”°ë¥¸ Agent ì„ íƒ"""
        selected = ["token_usage", "model_variance", "relevance"]
        
        if prompt_type == PromptType.TYPE_A:
            selected.extend(["information_density", "consistency", "hallucination"])
        elif prompt_type == PromptType.TYPE_B_TEXT:
            selected.append("information_density")
        elif prompt_type == PromptType.TYPE_B_IMAGE:
            selected.append("consistency")
        
        return selected
    
    async def _execute_workflow(
        self,
        agent_types: List[str],
        job_request: JobCreateRequest,
        execution_data: Dict[str, Any],
        execution_id: str
    ) -> Dict[str, Any]:
        """Workflow ì‹¤í–‰ ë° ê° Worker íƒ€ì´ë° ë¡œê¹…"""

        try:
            workflow_input = {
                "prompt": job_request.prompt,
                "prompt_type": job_request.prompt_type.value,
                "example_inputs": [inp.dict() for inp in job_request.example_inputs],
                "execution_results": execution_data["execution_results"],
                "embeddings": execution_data.get("embeddings", {}),
                "recommended_model": job_request.recommended_model.value if job_request.recommended_model else None
            }

            logger.info(f"[{execution_id}]    Executing {len(agent_types)} workers in parallel...")

            # ê° ì›Œì»¤ë³„ ê°œë³„ íƒ€ì´ë° ì¸¡ì •
            worker_timings = {}

            # ë©”íŠ¸ë¦­ ë²ˆí˜¸ ë§¤í•‘ (ìˆœì„œëŒ€ë¡œ 1-6)
            metric_numbers = {
                "token_usage": 1,
                "information_density": 2,
                "consistency": 3,
                "model_variance": 4,
                "hallucination": 5,
                "relevance": 6
            }

            # ë©”íŠ¸ë¦­ í•œêµ­ì–´ ì´ë¦„
            metric_names_kr = {
                "token_usage": "í† í° ì‚¬ìš©ëŸ‰",
                "information_density": "ì •ë³´ ë°€ë„",
                "consistency": "ì¼ê´€ì„±",
                "model_variance": "ëª¨ë¸ ë¶„ì‚°ë„",
                "hallucination": "í™˜ê° íƒì§€",
                "relevance": "ê´€ë ¨ì„±"
            }

            async def execute_single_agent(agent_type: str) -> tuple[str, Any]:
                """ë‹¨ì¼ ì—ì´ì „íŠ¸ ì‹¤í–‰ ë° íƒ€ì´ë° ë¡œê¹…"""
                metric_num = metric_numbers.get(agent_type, 0)
                metric_name_kr = metric_names_kr.get(agent_type, agent_type)
                worker_start = time.time()
                logger.info(f"[{execution_id}]    ğŸš€ Step 3-{metric_num}: [{agent_type}] ({metric_name_kr}) started")

                try:
                    result = await self.strands_core.execute_single_agent(agent_type, workflow_input)
                    worker_duration = time.time() - worker_start
                    worker_timings[agent_type] = worker_duration

                    score = result.get("score", 0.0) if result and result.get("success") else 0.0
                    logger.info(f"[{execution_id}]    âœ… Step 3-{metric_num} Complete: [{agent_type}] ({metric_name_kr}) - Score: {score:.2f} - {format_duration(worker_duration)}")

                    return agent_type, result
                except Exception as e:
                    worker_duration = time.time() - worker_start
                    logger.error(f"[{execution_id}]    âŒ Step 3-{metric_num} Failed: [{agent_type}] ({metric_name_kr}) - {format_duration(worker_duration)} - Error: {str(e)}")
                    return agent_type, None

            # ëª¨ë“  ì›Œì»¤ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
            results = await asyncio.gather(*[execute_single_agent(agent_type) for agent_type in agent_types])

            # ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            workflow_results = {agent_type: result for agent_type, result in results}

            # ì „ì²´ ì›Œì»¤ í†µê³„
            total_workers = len(agent_types)
            successful_workers = sum(1 for r in workflow_results.values() if r and r.get("success"))
            avg_duration = sum(worker_timings.values()) / len(worker_timings) if worker_timings else 0

            logger.info(f"[{execution_id}]    ğŸ“Š Workers Summary: {successful_workers}/{total_workers} succeeded - Avg: {format_duration(avg_duration)}")

            return workflow_results

        except Exception as e:
            logger.error(f"[{execution_id}] Workflow execution failed: {str(e)}")
            return {}
    
    async def _integrate_results(
        self,
        workflow_results: Dict[str, Any],
        prompt_type: PromptType
    ) -> tuple[float, Dict[str, float], Dict[str, Any]]:
        """ê²°ê³¼ í†µí•©"""
        
        metrics = {}
        
        for agent_type, result in workflow_results.items():
            if result and result.get("success"):
                score = result.get("score", 0.0)
                details = result.get("details", {})
                
                if agent_type == "token_usage":
                    metrics[agent_type] = TokenMetricScore(score=score, details=details)
                else:
                    metrics[agent_type] = MetricScore(score=score, details=details)
            else:
                metrics[agent_type] = None
        
        from app.agents.tools.tool_executor import ToolExecutor
        
        tool_executor = ToolExecutor(self.context)
        final_result = await tool_executor.execute_tool(
            "aggregate_metrics",
            {
                "prompt_type": prompt_type.value,
                "metrics": metrics
            }
        )
        
        final_score = final_result.get("final_score", 0.0) if final_result.get("success") else 0.0
        weighted_scores = final_result.get("weighted_scores", {}) if final_result.get("success") else {}
        
        return final_score, weighted_scores, metrics
    
    async def _generate_feedback(
        self,
        job_request: JobCreateRequest,
        metrics: Dict[str, Any],
        final_score: float
    ) -> Dict[str, Any]:
        """í”¼ë“œë°± ìƒì„±"""
        
        try:
            judge = self.context.get_judge()
            
            scores_summary = {}
            for metric_name, metric_result in metrics.items():
                if metric_result is not None:
                    scores_summary[metric_name] = metric_result.score
                else:
                    scores_summary[metric_name] = "ê³„ì‚°ë˜ì§€ ì•ŠìŒ"
            
            feedback_prompt = f"""ë‹¤ìŒ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…í•©ì ì¸ í”¼ë“œë°±ì„ ìƒì„±í•´ì£¼ì„¸ìš”.

í”„ë¡¬í”„íŠ¸: {job_request.prompt}
í”„ë¡¬í”„íŠ¸ íƒ€ì…: {job_request.prompt_type.value}
ìµœì¢… ì ìˆ˜: {final_score}/100

ê°œë³„ ì§€í‘œ ì ìˆ˜:
{chr(10).join([f"- {name}: {score}" for name, score in scores_summary.items()])}

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ í”¼ë“œë°±ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. ì „ë°˜ì  í‰ê°€
2. ê°•ì 
3. ì•½ì 
4. êµ¬ì²´ì  ê°œì„  ì œì•ˆ (3ê°€ì§€)

í•œêµ­ì–´ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."""

            feedback_text = await judge.analyze_text(feedback_prompt)
            
            return {
                "overall_feedback": feedback_text,
                "final_score": final_score,
                "individual_scores": scores_summary,
                "prompt_type": job_request.prompt_type.value
            }
            
        except Exception as e:
            logger.error(f"Feedback generation failed: {str(e)}")
            return {
                "overall_feedback": "í”¼ë“œë°± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "final_score": final_score,
                "error": str(e)
            }
    
    async def _prepare_execution_data(self, job_request: JobCreateRequest) -> Dict[str, Any]:
        """ê¸°ë³¸ ì‹¤í–‰ ë°ì´í„° ì¤€ë¹„"""
        from app.orchestrator.stages.run_stage import RunStage
        from app.orchestrator.stages.embed_stage import EmbedStage
        
        run_stage = RunStage(self.context)
        execution_results = await run_stage.execute(
            job_request.prompt,
            job_request.example_inputs,
            job_request.recommended_model,
            job_request.repeat_count,
            job_request.prompt_type
        )
        
        embed_stage = EmbedStage(self.context)
        embeddings = await embed_stage.execute(
            execution_results,
            job_request.example_inputs,
            job_request.prompt_type
        )
        
        return {
            "execution_results": execution_results,
            "embeddings": embeddings
        }

    async def _send_completion_email(
        self,
        job_request: JobCreateRequest,
        final_score: float,
        execution_id: str
    ):
        """í‰ê°€ ì™„ë£Œ ì´ë©”ì¼ ë°œì†¡"""
        try:
            # create_userì—ì„œ USER# prefix ì œê±°í•˜ì—¬ user_id ì¶”ì¶œ
            create_user = job_request.create_user
            if not create_user or not create_user.startswith("USER#"):
                logger.warning(f"[{execution_id}] âš ï¸ Invalid create_user format: {create_user}")
                return

            user_id = create_user.replace("USER#", "")
            logger.info(f"[{execution_id}] ğŸ“§ Preparing to send completion email for user_id: {user_id}")

            # 1. User IDë¡œ ì´ë©”ì¼ ì¡°íšŒ
            user_email = await self.user_repo.get_user_email(user_id)

            if not user_email:
                logger.warning(f"[{execution_id}] âš ï¸ User email not found for user_id: {user_id}")
                return

            logger.info(f"[{execution_id}] ğŸ“§ Sending completion email to {user_email}")

            # 2. prompt_id ì¶”ì¶œ (PROMPT#xxxì—ì„œ xxxë§Œ)
            prompt_id = None
            if job_request.PK and job_request.PK.startswith("PROMPT#"):
                prompt_id = job_request.PK.replace("PROMPT#", "")

            # 3. ì´ë©”ì¼ ë°œì†¡
            result = await self.ses_notifier.send_evaluation_complete_email(
                recipient_email=user_email,
                final_score=final_score,
                prompt_type=job_request.prompt_type.value,
                prompt_title=job_request.title,
                prompt_id=prompt_id
            )

            if result.get("success"):
                logger.info(f"[{execution_id}] âœ… Email sent successfully - MessageId: {result.get('message_id')}")
            else:
                logger.warning(f"[{execution_id}] âš ï¸ Email send failed: {result.get('error')}")

        except Exception as e:
            # ì´ë©”ì¼ ë°œì†¡ ì‹¤íŒ¨í•´ë„ í‰ê°€ ê²°ê³¼ëŠ” ë°˜í™˜ (non-critical)
            logger.error(f"[{execution_id}] Email notification failed (non-critical): {str(e)}")