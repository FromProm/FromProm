"""
Strands Framework Í∏∞Î∞ò Supervisor Agent
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional

from app.core.schemas import JobCreateRequest, EvaluationResult, PromptType, MetricScore, TokenMetricScore
from app.orchestrator.context import ExecutionContext
from app.agents.strands.agent_core import create_agent_core

logger = logging.getLogger(__name__)

class StrandsSupervisorAgent:
    """
    Strands Framework Í∏∞Î∞ò Supervisor Agent
    """
    
    def __init__(self, context: ExecutionContext):
        self.context = context
        self.strands_core = None
        self._initialize()
    
    def _initialize(self):
        """Ï¥àÍ∏∞Ìôî"""
        try:
            self.strands_core = create_agent_core()
            logger.info("üéØ Strands Supervisor initialized")
        except Exception as e:
            logger.error(f"Failed to initialize Strands: {str(e)}")
            raise
    
    async def evaluate_prompt(self, job_request: JobCreateRequest) -> EvaluationResult:
        """
        Strands Í∏∞Î∞ò ÌîÑÎ°¨ÌîÑÌä∏ ÌèâÍ∞Ä
        """
        logger.info(f"üéØ Strands Supervisor starting evaluation for: {job_request.prompt_type}")
        
        try:
            if not self.strands_core or not self.strands_core.is_available():
                raise Exception("Strands Agent Core not available")
            
            # 1. Í∏∞Î≥∏ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
            logger.info("üìã Step 1: Preparing execution data...")
            execution_data = await self._prepare_execution_data(job_request)
            
            # 2. Agent ÏÑ†ÌÉù
            logger.info("ü§ñ Step 2: Selecting agents...")
            agent_types = self._select_agents(job_request.prompt_type)
            
            # 3. Workflow Ïã§Ìñâ
            logger.info("‚ö° Step 3: Executing workflow...")
            workflow_results = await self._execute_workflow(
                agent_types, job_request, execution_data
            )
            
            # 4. Í≤∞Í≥º ÌÜµÌï©
            logger.info("üìä Step 4: Integrating results...")
            final_score, weighted_scores, metrics = await self._integrate_results(
                workflow_results, job_request.prompt_type
            )
            
            # 5. ÌîºÎìúÎ∞± ÏÉùÏÑ±
            logger.info("üí¨ Step 5: Generating feedback...")
            feedback = await self._generate_feedback(job_request, metrics, final_score)
            
            evaluation_result = EvaluationResult(
                final_score=final_score,
                weighted_scores=weighted_scores,
                execution_results=execution_data["execution_results"],
                feedback=feedback,
                **metrics
            )
            
            logger.info(f"‚úÖ Strands Supervisor completed - Final Score: {final_score}")
            return evaluation_result
            
        except Exception as e:
            logger.error(f"‚ùå Strands Supervisor failed: {str(e)}")
            raise
    
    def _select_agents(self, prompt_type: PromptType) -> List[str]:
        """ÌîÑÎ°¨ÌîÑÌä∏ ÌÉÄÏûÖÏóê Îî∞Î•∏ Agent ÏÑ†ÌÉù"""
        selected = ["token_usage", "model_variance", "relevance"]
        
        if prompt_type == PromptType.TYPE_A:
            selected.extend(["information_density", "consistency", "hallucination"])
        elif prompt_type == PromptType.TYPE_B_TEXT:
            selected.append("information_density")
        elif prompt_type == PromptType.TYPE_B_IMAGE:
            selected.append("consistency")
        
        return selected
    
    async def _execute_workflow(
        self,
        agent_types: List[str],
        job_request: JobCreateRequest,
        execution_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Workflow Ïã§Ìñâ"""
        
        try:
            workflow_input = {
                "prompt": job_request.prompt,
                "prompt_type": job_request.prompt_type.value,
                "example_inputs": [inp.dict() for inp in job_request.example_inputs],
                "execution_results": execution_data["execution_results"],
                "embeddings": execution_data.get("embeddings", {}),
                "recommended_model": job_request.recommended_model.value if job_request.recommended_model else ""
            }
            
            logger.info(f"   Executing {len(agent_types)} agents in parallel...")
            workflow_results = await self.strands_core.execute_agents_parallel(
                agent_types, 
                workflow_input
            )
            
            if workflow_results.get("success"):
                logger.info("   ‚úÖ Workflow completed successfully")
                return workflow_results["results"]
            else:
                logger.error(f"   ‚ùå Workflow failed: {workflow_results.get('error')}")
                return {}
                
        except Exception as e:
            logger.error(f"Workflow execution failed: {str(e)}")
            return {}
    
    async def _integrate_results(
        self,
        workflow_results: Dict[str, Any],
        prompt_type: PromptType
    ) -> tuple[float, Dict[str, float], Dict[str, Any]]:
        """Í≤∞Í≥º ÌÜµÌï©"""
        
        metrics = {}
        
        for agent_type, result in workflow_results.items():
            if result and result.get("success"):
                score = result.get("score", 0.0)
                details = result.get("details", {})
                
                if agent_type == "token_usage":
                    metrics[agent_type] = TokenMetricScore(score=score, details=details)
                else:
                    metrics[agent_type] = MetricScore(score=score, details=details)
            else:
                metrics[agent_type] = None
        
        from app.agents.tools.tool_executor import ToolExecutor
        
        tool_executor = ToolExecutor(self.context)
        final_result = await tool_executor.execute_tool(
            "aggregate_metrics",
            {
                "prompt_type": prompt_type.value,
                "metrics": metrics
            }
        )
        
        final_score = final_result.get("final_score", 0.0) if final_result.get("success") else 0.0
        weighted_scores = final_result.get("weighted_scores", {}) if final_result.get("success") else {}
        
        return final_score, weighted_scores, metrics
    
    async def _generate_feedback(
        self,
        job_request: JobCreateRequest,
        metrics: Dict[str, Any],
        final_score: float
    ) -> Dict[str, Any]:
        """ÌîºÎìúÎ∞± ÏÉùÏÑ±"""
        
        try:
            judge = self.context.get_judge()
            
            scores_summary = {}
            for metric_name, metric_result in metrics.items():
                if metric_result is not None:
                    scores_summary[metric_name] = metric_result.score
                else:
                    scores_summary[metric_name] = "Í≥ÑÏÇ∞ÎêòÏßÄ ÏïäÏùå"
            
            feedback_prompt = f"""Îã§Ïùå ÌîÑÎ°¨ÌîÑÌä∏Ïóê ÎåÄÌïú ÏÑ±Îä• ÌèâÍ∞Ä Í≤∞Í≥ºÎ•º Î∞îÌÉïÏúºÎ°ú Ï¢ÖÌï©Ï†ÅÏù∏ ÌîºÎìúÎ∞±ÏùÑ ÏÉùÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî.

ÌîÑÎ°¨ÌîÑÌä∏: {job_request.prompt}
ÌîÑÎ°¨ÌîÑÌä∏ ÌÉÄÏûÖ: {job_request.prompt_type.value}
ÏµúÏ¢Ö Ï†êÏàò: {final_score}/100

Í∞úÎ≥Ñ ÏßÄÌëú Ï†êÏàò:
{chr(10).join([f"- {name}: {score}" for name, score in scores_summary.items()])}

Îã§Ïùå ÌòïÏãùÏúºÎ°ú ÌîºÎìúÎ∞±ÏùÑ ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî:
1. Ï†ÑÎ∞òÏ†Å ÌèâÍ∞Ä
2. Í∞ïÏ†ê
3. ÏïΩÏ†ê
4. Íµ¨Ï≤¥Ï†Å Í∞úÏÑ† Ï†úÏïà (3Í∞ÄÏßÄ)

ÌïúÍµ≠Ïñ¥Î°ú ÏûëÏÑ±Ìï¥Ï£ºÏÑ∏Ïöî."""

            feedback_text = await judge.analyze_text(feedback_prompt)
            
            return {
                "overall_feedback": feedback_text,
                "final_score": final_score,
                "individual_scores": scores_summary,
                "prompt_type": job_request.prompt_type.value
            }
            
        except Exception as e:
            logger.error(f"Feedback generation failed: {str(e)}")
            return {
                "overall_feedback": "ÌîºÎìúÎ∞± ÏÉùÏÑ± Ï§ë Ïò§Î•òÍ∞Ä Î∞úÏÉùÌñàÏäµÎãàÎã§.",
                "final_score": final_score,
                "error": str(e)
            }
    
    async def _prepare_execution_data(self, job_request: JobCreateRequest) -> Dict[str, Any]:
        """Í∏∞Î≥∏ Ïã§Ìñâ Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ"""
        from app.orchestrator.stages.run_stage import RunStage
        from app.orchestrator.stages.embed_stage import EmbedStage
        
        run_stage = RunStage(self.context)
        execution_results = await run_stage.execute(
            job_request.prompt,
            job_request.example_inputs,
            job_request.recommended_model,
            job_request.repeat_count,
            job_request.prompt_type
        )
        
        embed_stage = EmbedStage(self.context)
        embeddings = await embed_stage.execute(
            execution_results,
            job_request.example_inputs,
            job_request.prompt_type
        )
        
        return {
            "execution_results": execution_results,
            "embeddings": embeddings
        }