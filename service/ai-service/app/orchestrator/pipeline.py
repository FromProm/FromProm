import asyncio
import logging
from typing import Dict, Any, List
from datetime import datetime

from opentelemetry import trace

from app.core.schemas import JobCreateRequest, EvaluationResult, MetricScore, PromptType
from app.orchestrator.context import ExecutionContext
from app.orchestrator.stages.run_stage import RunStage
from app.orchestrator.stages.token_stage import TokenStage
from app.orchestrator.stages.density_stage import DensityStage
from app.orchestrator.stages.embed_stage import EmbedStage
from app.orchestrator.stages.consistency_stage import ConsistencyStage
from app.orchestrator.stages.relevance_stage import RelevanceStage
from app.orchestrator.stages.variance_stage import VarianceStage
from app.orchestrator.stages.judge_stage import JudgeStage
from app.orchestrator.stages.aggregate_stage import AggregateStage
from app.orchestrator.stages.feedback_stage import FeedbackStage
from app.core.config import settings

logger = logging.getLogger(__name__)

class Orchestrator:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„° - ìœ ì¼í•œ ì§„ì‹¤ì˜ ì†ŒìŠ¤"""
    
    def __init__(self, context: ExecutionContext):
        self.context = context
        self.tracer = trace.get_tracer(__name__)
        self.stages = {
            'run': RunStage(context),
            'token': TokenStage(context),
            'density': DensityStage(context),
            'embed': EmbedStage(context),
            'consistency': ConsistencyStage(context),
            'relevance': RelevanceStage(context),
            'variance': VarianceStage(context),
            'judge': JudgeStage(context),
            'aggregate': AggregateStage(context),
            'feedback': FeedbackStage(context)
        }
    
    async def run(self, job_request: JobCreateRequest) -> EvaluationResult:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ë³‘ë ¬ ì²˜ë¦¬)"""
        logger.info("=" * 80)
        logger.info("ğŸš€ [PIPELINE] í‰ê°€ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        logger.info(f"ğŸ“ í”„ë¡¬í”„íŠ¸ íƒ€ì…: {job_request.prompt_type}")
        logger.info(f"ğŸ“Š ì˜ˆì œ ê°œìˆ˜: {len(job_request.example_inputs)}")
        logger.info(f"ğŸ” ë°˜ë³µ íšŸìˆ˜: {job_request.repeat_count}")
        logger.info("=" * 80)
        
        try:
            # [1ë‹¨ê³„] í”„ë¡¬í”„íŠ¸ ì‹¤í–‰ - ì¶œë ¥ ìƒì„± + Variance ëª¨ë¸ ì‹¤í–‰ (ì„ í–‰ í•„ìˆ˜)
            logger.info("ğŸ”¹ [STAGE 1/6] RunStage ì‹œì‘...")
            with self.tracer.start_as_current_span('RunStage'):
                execution_results = await self.stages['run'].execute(
                    job_request.prompt,
                    job_request.example_inputs,
                    job_request.recommended_model,
                    job_request.repeat_count,
                    job_request.prompt_type
                )
            logger.info(f"âœ… [STAGE 1/6] RunStage ì™„ë£Œ - {len(execution_results.get('executions', []))}ê°œ ì‹¤í–‰")
            
            # ì‹¤í–‰ ê²°ê³¼ ë³´ì¡´ (S3 ì €ì¥ìš©)
            self._last_execution_results = execution_results
            
            # [2ë‹¨ê³„] ì„ë² ë”© + ë…ë¦½ ì§€í‘œë“¤ ë³‘ë ¬ ì‹¤í–‰
            # ì„ë² ë”©ì€ ì¼ê´€ì„± ê³„ì‚°ì— í•„ìš”í•˜ë¯€ë¡œ í•¨ê»˜ ì‹¤í–‰
            logger.info("ğŸ”¹ [STAGE 2/6] ë³‘ë ¬ ì§€í‘œ ê³„ì‚° ì‹œì‘...")
            logger.info(f"   ë³‘ë ¬ ì‹¤í–‰ ëŒ€ìƒ: TokenStage, DensityStage, EmbedStage, RelevanceStage, JudgeStage, VarianceStage")
            parallel_tasks = []
            task_names = []
            
            # í† í° ê³„ì‚° (í•­ìƒ)
            parallel_tasks.append(
                self._execute_with_xray('TokenStage', self.stages['token'].execute, job_request.prompt, execution_results)
            )
            task_names.append('token')
            
            # ì •ë³´ ë°€ë„ (TYPE_A, TYPE_B_TEXT)
            if job_request.prompt_type in [PromptType.TYPE_A, PromptType.TYPE_B_TEXT]:
                parallel_tasks.append(
                    self._execute_with_xray('DensityStage', self.stages['density'].execute, execution_results)
                )
                task_names.append('density')
            
            # ì„ë² ë”© ìƒì„± (ì¼ê´€ì„± ê³„ì‚°ìš©)
            parallel_tasks.append(
                self._execute_with_xray('EmbedStage', self.stages['embed'].execute, execution_results, job_request.example_inputs, job_request.prompt_type)
            )
            task_names.append('embed')
            
            # ì •í™•ë„ ê³„ì‚° (ëª¨ë“  íƒ€ì…)
            parallel_tasks.append(
                self._execute_with_xray('RelevanceStage', self.stages['relevance'].execute, job_request.prompt, job_request.example_inputs, execution_results, job_request.prompt_type)
            )
            task_names.append('relevance')
            
            # í™˜ê° íƒì§€ (TYPE_Aë§Œ)
            if job_request.prompt_type == PromptType.TYPE_A:
                parallel_tasks.append(
                    self._execute_with_xray('JudgeStage', self.stages['judge'].execute, job_request.example_inputs, execution_results)
                )
                task_names.append('judge')
            
            # ëª¨ë¸ë³„ í¸ì°¨ (ëª¨ë“  íƒ€ì…) - ê¸°ì¡´ ì¶œë ¥ ì¬ì‚¬ìš©
            parallel_tasks.append(
                self._execute_with_xray('VarianceStage', self.stages['variance'].execute, job_request.prompt, job_request.example_inputs, job_request.prompt_type, job_request.recommended_model, execution_results)
            )
            task_names.append('variance')
            
            # ë³‘ë ¬ ì‹¤í–‰
            logger.info(f"   ì‹¤í–‰ ì¤‘: {', '.join(task_names)}")
            parallel_results = await asyncio.gather(*parallel_tasks, return_exceptions=True)
            
            # ê²°ê³¼ ë§¤í•‘
            results_map = {}
            for name, result in zip(task_names, parallel_results):
                if isinstance(result, Exception):
                    logger.error(f"   âŒ {name} ì‹¤íŒ¨: {str(result)}")
                    results_map[name] = None
                else:
                    score_str = f"ì ìˆ˜: {result.score:.2f}" if hasattr(result, 'score') else "ì™„ë£Œ"
                    logger.info(f"   âœ… {name} {score_str}")
                    results_map[name] = result
            
            logger.info("âœ… [STAGE 2/6] ë³‘ë ¬ ì§€í‘œ ê³„ì‚° ì™„ë£Œ")
            
            # ê²°ê³¼ ì¶”ì¶œ
            token_score = results_map.get('token')
            density_score = results_map.get('density')
            embeddings = results_map.get('embed')
            relevance_score = results_map.get('relevance')
            hallucination_score = results_map.get('judge')
            variance_score = results_map.get('variance')
            
            # [3ë‹¨ê³„] ì¼ê´€ì„± ê³„ì‚° (ì„ë² ë”© ì™„ë£Œ í›„)
            consistency_score = None
            if job_request.prompt_type in [PromptType.TYPE_A, PromptType.TYPE_B_IMAGE]:
                logger.info("ğŸ”¹ [STAGE 3/6] ConsistencyStage ì‹œì‘...")
                if embeddings and 'outputs' in embeddings:
                    with self.tracer.start_as_current_span('ConsistencyStage'):
                        consistency_score = await self.stages['consistency'].execute(
                            embeddings['outputs']
                        )
                    logger.info(f"âœ… [STAGE 3/6] ConsistencyStage ì™„ë£Œ - ì ìˆ˜: {consistency_score.score:.2f}")
                else:
                    logger.warning("âš ï¸ [STAGE 3/6] ConsistencyStage ìŠ¤í‚µ - ì„ë² ë”© ë°ì´í„° ì—†ìŒ")
            
            # [4ë‹¨ê³„] ìµœì¢… ì ìˆ˜ ì§‘ê³„
            logger.info("ğŸ”¹ [STAGE 4/6] AggregateStage ì‹œì‘...")
            with self.tracer.start_as_current_span('AggregateStage'):
                final_result = await self.stages['aggregate'].execute(
                    job_request.prompt_type,
                    {
                        'token_usage': token_score,
                        'information_density': density_score,
                        'consistency': consistency_score,
                        'relevance': relevance_score,
                        'hallucination': hallucination_score,
                        'model_variance': variance_score
                    }
                )
            logger.info(f"âœ… [STAGE 4/6] AggregateStage ì™„ë£Œ - ìµœì¢… ì ìˆ˜: {final_result.final_score:.2f}")
            
            # ì‹¤ì œ AI ì¶œë ¥ ê²°ê³¼ í¬í•¨
            final_result.execution_results = execution_results
            
            # [5ë‹¨ê³„] í”„ë¡¬í”„íŠ¸ ê°œì„  í”¼ë“œë°± ìƒì„±
            logger.info("ğŸ”¹ [STAGE 5/6] FeedbackStage ì‹œì‘...")
            try:
                evaluation_data = {
                    'token_usage': {'score': token_score.score if token_score else 0} if token_score else None,
                    'information_density': {'score': density_score.score if density_score else 0} if density_score else None,
                    'consistency': {'score': consistency_score.score if consistency_score else 0} if consistency_score else None,
                    'relevance': {'score': relevance_score.score if relevance_score else 0} if relevance_score else None,
                    'hallucination': {'score': hallucination_score.score if hallucination_score else 0} if hallucination_score else None,
                    'model_variance': {'score': variance_score.score if variance_score else 0} if variance_score else None,
                    'execution_results': execution_results
                }
                
                with self.tracer.start_as_current_span('FeedbackStage'):
                    feedback = await self.stages['feedback'].execute(
                        evaluation_data,
                        prompt=job_request.prompt,
                        prompt_type=job_request.prompt_type,
                        example_inputs=job_request.example_inputs
                    )
                final_result.feedback = feedback
                logger.info("âœ… [STAGE 5/6] FeedbackStage ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"âš ï¸ [STAGE 5/6] FeedbackStage ì‹¤íŒ¨: {str(e)}")
                final_result.feedback = {'error': str(e)}
            
            logger.info("=" * 80)
            logger.info(f"ğŸ‰ [PIPELINE] í‰ê°€ ì™„ë£Œ - ìµœì¢… ì ìˆ˜: {final_result.final_score:.2f}")
            logger.info("=" * 80)
            return final_result
            
        except Exception as e:
            logger.error(f"Pipeline execution failed: {str(e)}")
            raise
    
    async def _execute_with_xray(self, segment_name: str, func, *args):
        """OpenTelemetry spanìœ¼ë¡œ ê°ì‹¸ì„œ ì‹¤í–‰"""
        with self.tracer.start_as_current_span(segment_name):
            return await func(*args)