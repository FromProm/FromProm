#!/usr/bin/env python3
"""
Strands Supervisor Pattern ì‹¤ì œ í‰ê°€ í…ŒìŠ¤íŠ¸
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

import asyncio

async def test_evaluation():
    try:
        print("ğŸ¯ Strands Supervisor Pattern ì‹¤ì œ í‰ê°€ í…ŒìŠ¤íŠ¸")
        
        from app.agents.agent_pipeline import AgentPipeline
        from app.orchestrator.context import ExecutionContext
        from app.core.schemas import JobCreateRequest, ExampleInput, PromptType, RecommendedModel
        
        # Mock JobCreateRequest ìƒì„±
        job_request = JobCreateRequest(
            prompt="í•œêµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
            example_inputs=[
                ExampleInput(content="í•œêµ­ì˜ ìˆ˜ë„ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”", input_type="text")
            ],
            prompt_type=PromptType.TYPE_B_TEXT,
            recommended_model=RecommendedModel.CLAUDE_SONNET_4_5,
            repeat_count=1,
            title="Strands í…ŒìŠ¤íŠ¸",
            description="Supervisor Pattern í…ŒìŠ¤íŠ¸",
            user_id="test_user"
        )
        
        # ExecutionContext ë° Pipeline ìƒì„±
        context = ExecutionContext()
        pipeline = AgentPipeline(context)
        
        print("ğŸš€ Strands Supervisor Pattern ì‹¤í–‰ ì¤‘...")
        print(f"   í”„ë¡¬í”„íŠ¸: {job_request.prompt}")
        print(f"   íƒ€ì…: {job_request.prompt_type.value}")
        
        # ì‹¤ì œ í‰ê°€ ì‹¤í–‰
        result = await pipeline.run(job_request)
        
        print(f"\nâœ… í‰ê°€ ì™„ë£Œ!")
        print(f"   ìµœì¢… ì ìˆ˜: {result.final_score}")
        
        if result.weighted_scores:
            print(f"   ê°€ì¤‘ ì ìˆ˜:")
            for metric, score in result.weighted_scores.items():
                print(f"     - {metric}: {score}")
        
        # ê°œë³„ Worker ê²°ê³¼ í™•ì¸
        print(f"\nğŸ“Š Worker ê²°ê³¼:")
        metrics = {
            'token_usage': result.token_usage,
            'information_density': result.information_density,
            'consistency': result.consistency,
            'model_variance': result.model_variance,
            'relevance': result.relevance,
            'hallucination': result.hallucination
        }
        
        for name, metric in metrics.items():
            if metric:
                worker_type = "AI Worker" if name == "hallucination" else "Tool Worker"
                print(f"   {name}: {metric.score} ({worker_type})")
            else:
                print(f"   {name}: ê³„ì‚°ë˜ì§€ ì•ŠìŒ")
        
        # í”¼ë“œë°± í™•ì¸
        if hasattr(result, 'feedback') and result.feedback:
            print(f"\nğŸ’¬ Supervisor í”¼ë“œë°±:")
            feedback = result.feedback
            if isinstance(feedback, dict) and 'overall_feedback' in feedback:
                feedback_preview = feedback['overall_feedback'][:150] + "..." if len(feedback['overall_feedback']) > 150 else feedback['overall_feedback']
                print(f"   {feedback_preview}")
        
        print(f"\nğŸ‰ Strands Supervisor Pattern í…ŒìŠ¤íŠ¸ ì„±ê³µ!")
        print(f"   - Claude Supervisor: ì‘ì—… ë¶„ë°° ë° í†µí•© âœ…")
        print(f"   - Tool Workers: ê³„ì‚° ì‘ì—… ìˆ˜í–‰ âœ…")
        print(f"   - AI Worker: í™˜ê° íƒì§€ ìˆ˜í–‰ âœ…")
        print(f"   - ìµœì¢… ê²°ê³¼: {result.final_score}ì  âœ…")
        
    except Exception as e:
        print(f"âŒ í‰ê°€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_evaluation())