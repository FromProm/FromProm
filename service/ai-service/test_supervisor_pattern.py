#!/usr/bin/env python3
"""
Supervisor Pattern í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
"""

import asyncio
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_supervisor_pattern():
    """Supervisor Pattern í…ŒìŠ¤íŠ¸"""
    try:
        from app.orchestrator.context import ExecutionContext
        from app.agents.agent_pipeline import AgentPipeline
        from app.core.schemas import JobCreateRequest, ExampleInput, PromptType, RecommendedModel
        
        print("ğŸ¯ Supervisor Pattern í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        # Mock JobCreateRequest
        job_request = JobCreateRequest(
            prompt="í•œêµ­ì˜ ê²½ì œ ìƒí™©ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
            example_inputs=[
                ExampleInput(content="í•œêµ­ ê²½ì œì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”", input_type="text")
            ],
            prompt_type=PromptType.TYPE_A,
            recommended_model=RecommendedModel.CLAUDE_SONNET_4_5,
            repeat_count=2,
            title="Supervisor Pattern í…ŒìŠ¤íŠ¸",
            description="6ê°œ ì „ë¬¸ AI Agent í…ŒìŠ¤íŠ¸",
            user_id="supervisor_test"
        )
        
        # ExecutionContext ìƒì„±
        context = ExecutionContext()
        
        # Agent Pipeline (Supervisor Pattern) ì‹¤í–‰
        pipeline = AgentPipeline(context)
        
        print("ğŸ“Š Supervisor Pattern ì •ë³´:")
        supervisor_info = pipeline.get_supervisor_info()
        for key, value in supervisor_info.items():
            print(f"   {key}: {value}")
        
        print("\nğŸš€ Supervisor Pattern ì‹¤í–‰ ì¤‘...")
        result = await pipeline.run(job_request)
        
        print(f"\nâœ… Supervisor Pattern ì‹¤í–‰ ì™„ë£Œ!")
        print(f"   ìµœì¢… ì ìˆ˜: {result.final_score}")
        print(f"   ê°€ì¤‘ ì ìˆ˜: {result.weighted_scores}")
        
        # ê°œë³„ ì§€í‘œ í™•ì¸
        metrics = {
            'token_usage': result.token_usage,
            'information_density': result.information_density,
            'consistency': result.consistency,
            'model_variance': result.model_variance,
            'hallucination': result.hallucination,
            'relevance': result.relevance
        }
        
        print(f"\nğŸ“Š ì „ë¬¸ AI Agent ê²°ê³¼:")
        for name, metric in metrics.items():
            if metric:
                print(f"   {name}: {metric.score} (Agent: {metric.details.get('agent', 'Unknown')})")
            else:
                print(f"   {name}: None âŒ")
        
        # í”¼ë“œë°± í™•ì¸
        if hasattr(result, 'feedback') and result.feedback:
            print(f"\nğŸ’¬ Supervisor í”¼ë“œë°±:")
            feedback = result.feedback
            if isinstance(feedback, dict):
                for key, value in feedback.items():
                    if key == 'overall_feedback':
                        print(f"   ì¢…í•© í”¼ë“œë°±: {value[:200]}...")
                    else:
                        print(f"   {key}: {value}")
        
    except Exception as e:
        print(f"âŒ Supervisor Pattern í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        print(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤:")
        print(traceback.format_exc())

async def main():
    print("=" * 60)
    print("ğŸ¯ Supervisor Pattern í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    await test_supervisor_pattern()

if __name__ == "__main__":
    asyncio.run(main())